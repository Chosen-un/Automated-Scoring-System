# AUTOMATED SCORING SYSTEM

**ABSTRACT**

This research addresses the need for a transparent, consistent, and pedagogically effective automated scoring system for open-ended student responses. While traditional automated essay scoring (AES) methods often rely on feature engineering or black-box models, they frequently lack the capacity for providing interpretable rationales and constructive feedback, which are essential for student learning. This study proposes and implements a novel Retrieval-Augmented Generation (RAG) framework to bridge this gap. Our methodology employs a dual-corpus approach, leveraging both a curated dataset of teacher-graded examples and a domain-specific factual corpus to ground a large language model (LLM) in specific context and content.

The system is engineered as a multi-stage pipeline orchestrated by LangGraph. When a new student submission is received, a query embedding is generated using the state-of-the-art `Sentence-Transformers/all-mpnet-base-v2` model. This embedding is then used to perform a semantic search against two separate FAISS vector stores. The first vector store, populated with human-graded teacher examples, enables a dynamic few-shot learning process, allowing the LLM to learn the nuances of human-like grading and feedback. The second store, containing a factual corpus on sustainable development, provides an external knowledge base to mitigate hallucination and ensure factual accuracy.

The retrieved information, along with the student's answer and a dynamically generated rubric, is compiled into a highly-structured prompt for a second, high-performance LLM, the `Llama-4-Maverick-17B` model, accessed via the Groq API. This LLM serves as the grading engine, producing a detailed output. This output includes a granular score breakdown for each rubric criterion, a rationale explaining the score for the teacher, and a student-facing feedback message structured with clear strengths, areas for improvement, and encouraging advice. This multi-LLM, RAG-based approach is demonstrated to provide accurate and explainable scores while offering rich, actionable feedback, thereby contributing to the development of a more transparent and educationally meaningful LLM-driven assessment framework.

